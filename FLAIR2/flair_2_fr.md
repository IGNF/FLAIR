---
title: D√©fis Flair
---
üá¨üáß <a style="font-size: 11pt" href="./flair_2.html"><b>English version</b></a>
üîô <a style="font-size: 11pt" href="../index.html"><b>Retour FLAIR</b></a>

# Welcome to IGN's FLAIR datasets page!

<p align="center"><img src="../img/flair_bandeau.jpg" alt="" style="width:100%;max-width:1200px;" /></p>


<br>
## FLAIR #2 : Information texturale et temporelle √† partir d'imagerie optique multimodal pour la segmentation s√©mantique üåçüå±üè†üå≥‚û°Ô∏èüõ©Ô∏èüõ∞Ô∏è
 
Challenge organis√© par l'IGN avec le soutient du <a href="https://cnes.fr/en" target="_blank"><b>CNES</b></a> et de <a href="https://www.connectbycnes.fr/" target="_blank"><b>Connect by CNES</b></a> dans le cadre d'un projet Copernicus / FPCUP.<br>
<img style="width:100%;max-width:800px;" src="../img/flair-2_logos-hd.png" alt="Drawing"/>
<br/><br/><br/>

>FLAIR #2 <b>datapaper  &#128209;</b>  : https://arxiv.org/pdf/2305.14467.pdf <br/>
>FLAIR #2 <b>NeurIPS datapaper  &#128209;</b>  : https://proceedings.neurips.cc/paper_files/paper/2023/file/353ca88f722cdd0c481b999428ae113a-Paper-Datasets_and_Benchmarks.pdf <br/>
>FLAIR #2 <b>NeurIPS poster  &#128209;</b>  : https://neurips.cc/media/PosterPDFs/NeurIPS%202023/73621.png?t=1699528363.252194 <br/>
>FLAIR #2 <b>d√©p√¥t GitHub &#128193;</b> : https://github.com/IGNF/FLAIR-2 <br/>
>FLAIR #2 <b>page du d√©fi &#128187;</b> : https://codalab.lisn.upsaclay.fr/competitions/13447 [ferm√©]

><b>Mod√®les pr√©-entra√Æn√©s</b> : pour l'instant sur demande ! 

<br/><br/>


<details><summary><font size=3.5>‚ñ∂Ô∏è Contexte du challenge</font><em> (cliquer pour agrandir)</em></summary> 

Avec ce nouveau d√©fi, les participants auront pour t√¢che de d√©velopper des solutions innovantes qui peuvent exploiter efficacement les informations texturales des images a√©riennes prises √† une seule date, ainsi que les informations temporelles/spectrales provenant des s√©ries temporelles des satellites Sentinel-2, afin d'am√©liorer la segmentation s√©mantique, l'adaptation de domaine et l'apprentissage par transfert. Vos solutions devraient relever les d√©fis li√©s √† la conciliation des diff√©rentes p√©riodes d'acquisition, des r√©solutions spatiales diff√©rentes, de l'adaptation aux conditions d'acquisitions variables et de la gestion de l'h√©t√©rog√©n√©it√© des classes s√©mantiques.


<br/><br/>
<p align="center"><img src="../img/flair_2_frise.png" alt="" style="width:100%;max-width:1000px;"/></p><br>

</details>
<br>

<details><summary><font size=3.5>‚ñ∂Ô∏è Description du dataset</font> <em> (cliquer pour agrandir)</em></summary> 

Le jeu de donn√©es FLAIR #2 comprend 20,384,841,728 pixels annot√©s avec une r√©solution spatiale de 0,20 m √† partir d'images a√©riennes, r√©partis en 77,762 patchs de taille 512x512. Le jeu de donn√©es FLAIR #2 inclut √©galement une vaste collection de donn√©es satellites, avec un total de 51,244 acquisitions d'images des satellites Copernicus Sentinel-2. Pour chaque zone, les acquisitions sur un an ont √©t√© retenues, offrant des informations pr√©cieuses sur la dynamique spatio-temporelle et les caract√©ristiques spectrales de la couverture terrestre. En raison de la diff√©rence significative de r√©solution spatiale entre les images a√©riennes et les donn√©es satellites, les zones initialement d√©finies manquent de contexte suffisant car elles ne sont compos√©es que de quelques pixels Sentinel-2. Pour rem√©dier √† cela, une marge a √©t√© appliqu√©e pour cr√©er des zones plus grandes appel√©es super-zones. Cela garantit que chaque patch du jeu de donn√©es est associ√© √† une super-zone de donn√©es Sentinel-2 de taille suffisante, offrant un niveau minimum de contexte provenant du satellite.<br><br/>

<p align="center"><img src="../img/flair_2_spatial.png" alt="" style="width:100%;max-width:400px;"/></p><br>

Le jeu de donn√©es couvre 50 domaines spatial, comprenant 916 zones r√©parties sur 817 km¬≤. Avec 13 classes s√©mantiques (plus 6 non utilis√©es dans ce d√©fi), ce jeu de donn√©es constitue une base solide pour faire progresser les techniques de cartographie de la couverture du sol.<br><br>

<center>
<table style="width:80%;max-width:700px;">
<thead>
  <tr><th width=7%></th><th>Class</th><th style='text-align: center' width=15%>Value</th><th style='text-align: center'>Freq.-train (%)</th><th style='text-align: center'>Freq.-test (%)</th></tr>
</thead>
<tbody>
  <tr><td bgcolor='#db0e9a'></td><td>building</td><td style='text-align: center'>1</td><td style='text-align: center'>8.14</td><td style='text-align: center'>3.26</td></tr>
  
  <tr><td bgcolor='#938e7b'></td><td>pervious surface</td><td style='text-align: center'>2</td><td style='text-align: center'>8.25</td><td style='text-align: center'>3.82</td></tr>
  
  <tr><td bgcolor='#f80c00'></td><td>impervious surface</td><td style='text-align: center'>3</td><td style='text-align: center'>13.72</td><td style='text-align: center'>5.87</td></tr>
  
  <tr><td bgcolor='#a97101'></td><td>bare soil</td><td style='text-align: center'>4</td><td style='text-align: center'>3.47</td><td style='text-align: center'>1.6</td></tr>
  
  <tr><td bgcolor='#1553ae'></td><td>water</td><td style='text-align: center'>5</td><td style='text-align: center'>4.88</td><td style='text-align: center'>3.17</td></tr>
  
  <tr><td bgcolor='#194a26'></td><td>coniferous</td><td style='text-align: center'>6</td><td style='text-align: center'>2.74</td><td style='text-align: center'>10.24</td></tr>
  
  <tr><td bgcolor='#46e483'></td><td>deciduous</td><td style='text-align: center'>7</td><td style='text-align: center'>15.38</td><td style='text-align: center'>24.79</td></tr>
  
  <tr><td bgcolor='#f3a60d'></td><td>brushwood</td><td style='text-align: center'>8</td><td style='text-align: center'>6.95</td><td style='text-align: center'>3.81</td></tr>
  
  <tr><td bgcolor='#660082'></td><td>vineyard</td><td style='text-align: center'>9</td><td style='text-align: center'>3.13</td><td style='text-align: center'>2.55</td></tr>
  
  <tr><td bgcolor='#55ff00'></td><td>herbaceous vegetation</td><td style='text-align: center'>10</td><td style='text-align: center'>17.84</td><td style='text-align: center'>19.76</td></tr>
  
  <tr><td bgcolor='#fff30d'></td><td>agricultural land</td><td style='text-align: center'>11</td><td style='text-align: center'>10.98</td><td style='text-align: center'>18.19</td></tr>
  
  <tr><td bgcolor='#e4df7c'></td><td>plowed land</td><td style='text-align: center'>12</td><td style='text-align: center'>3.88</td><td style='text-align: center'>1.81</td></tr>
  
  <tr><td bgcolor='#3de6eb'></td><td>swimming pool</td><td style='text-align: center'>13</td><td style='text-align: center'>0.01</td><td style='text-align: center'>0.02</td></tr>
  
  <tr><td bgcolor='#ffffff'></td><td>snow</td><td style='text-align: center'>14</td><td style='text-align: center'>0.15</td><td style='text-align: center'>-</td></tr>
  
  <tr><td bgcolor='#8ab3a0'></td><td>clear cut</td><td style='text-align: center'>15</td><td style='text-align: center'>0.15</td><td style='text-align: center'>0.82</td></tr>
  
  <tr><td bgcolor='#6b714f'></td><td>mixed</td><td style='text-align: center'>16</td><td style='text-align: center'>0.05</td><td style='text-align: center'>0.12</td></tr>
  
  <tr><td bgcolor='#c5dc42'></td><td>ligneous</td><td style='text-align: center'>17</td><td style='text-align: center'>0.01</td><td style='text-align: center'>-</td></tr>
  
  <tr><td bgcolor='#9999ff'></td><td>greenhouse</td><td style='text-align: center'>18</td><td style='text-align: center'>0.12</td><td style='text-align: center'>0.15</td></tr>
  
  <tr><td bgcolor='#000000'></td><td>other</td><td style='text-align: center'>19</td><td style='text-align: center'>0.14</td><td style='text-align: center'>0.04</td></tr>
</tbody>
</table>
</center>
<br><br>
</details>
<br>

<details><summary><font size=3.5>‚ñ∂Ô∏è Mod√®le de r√©f√©rence (baseline): U-T&T</font> <em> (cliquer pour agrandir)</em></summary> 

Nous proposons le mod√®le U-T&T, une architecture √† deux branches qui combine les informations spatiales et temporelles √† partir d'images a√©riennes tr√®s haute r√©solution et d'images satellites haute r√©solution en une seule sortie. L'architecture U-Net est utilis√©e pour la branche spatiale/texture, en utilisant un mod√®le avec un encodeur ResNet34 pr√©-entra√Æn√© sur ImageNet. Pour la branche spatio-temporelle, l'architecture U-TAE int√®gre un Encodeur √† Attention Temporelle (TAE) pour explorer les caract√©ristiques spatiales et temporelles des s√©ries temporelles de Sentinel-2, en appliquant des masques d'attention √† diff√©rentes r√©solutions lors du d√©codage. Ce mod√®le permet la fusion des informations apprises √† partir des deux sources.
<br><br>
</details>
<br>

<details><summary><font size=3.5>‚ñ∂Ô∏è T√©l√©chargement du dataset</font> <em>(cliquer pour agrandir)</em></summary> 

Pour l'instant sur inscription au d√©fi ! <br>

<center>
<table style="width:80%;max-width:700px;">
  <tr>
    <th>Donn√©es</th><th>Volume</th><th>Type</th><th>Lien</th>
 </tr>
 <tr><td colspan="4" height = 10px></td>
 </tr>
  <tr>
    <td>Images a√©riennes - entra√Ænement</td><td>50.7 Go</td><td>.zip</td>
    <td>-</td>
  </tr>
   <tr>
    <td>Images a√©riennes - test</td><td>13.4 Go</td><td>.zip</td>
    <td>-</td>
  </tr>
 </tr>
  <tr>
    <td>Images Sentinel-2 - entra√Ænement</td><td>22.8 Go</td><td>.zip</td>
    <td>-</td>
  </tr>
   <tr>
    <td>Images Sentinel-2 - test</td><td>6 Go</td><td>.zip</td>
    <td>-</td>
  </tr>
  <tr>
    <td>Annotations - entra√Ænement</td><td>485 Mo</td><td>.zip</td>
    <td>-</td>
  </tr>
  <tr style="cellspacing: 10px">
    <td>Annotations - test</td><td>108 Mo</td><td>.zip</td>
    <td>-</td>
  </tr>
  <tr><td colspan="4" height=10px style="outline: "></td>
  <tr>
    <td>M√©tadonn√©es a√©riennes</td><td>16.1 Mo</td><td>.json</td>
    <td>-</td>
  </tr>
  <tr>
    <td>Correspondance images a√©riennes <-> Sentinel-2</td><td>16.1 Mo</td><td>.json</td>
    <td>-</td>
  </tr>
  <tr>
    <td>Shapefile zones</td><td>392 Ko</td><td>.gpkg</td>
    <td>-</td>
  </tr>
  <tr><td colspan="4" height=10px style="outline: "></td>
  <tr>
    <td>Jeu de donn√©es exemple (entra√Ænement et test r√©duits)</td><td>1.6 Go/td><td>.zip</td>
    <td><a style="font-size: 10pt" href="https://storage.gra.cloud.ovh.net/v1/AUTH_366279ce616242ebb14161b7991a8461/defi-ia/flair_data_2/flair_2_toy_dataset.zip"><b>download</b></a></td>
  </tr>
</table>
</center>
<br/><br/>

</details>
<br><br>

### Citation

Si vous utilisez des donn√©es de FLAIR #2, merci d'inclure la citation suivante:

Texte brut:
```
Anatol Garioud, Nicolas Gonthier, Loic Landrieu, Apolline De Wit, Marion Valette, Marc Poup√©e, S√©bastien Giordano and Boris Wattrelos. 2023. 
FLAIR: a Country-Scale Land Cover Semantic Segmentation Dataset From Multi-Source Optical Imagery. (2023). 
DOI: https://doi.org/10.48550/arXiv.2310.13336
```

BibTex:
```
@inproceedings{ign2023flair2,
      title={FLAIR: a Country-Scale Land Cover Semantic Segmentation Dataset From Multi-Source Optical Imagery}, 
      author={Anatol Garioud and Nicolas Gonthier and Loic Landrieu and Apolline De Wit and Marion Valette and Marc Poup√©e and S√©bastien Giordano and Boris Wattrelos},
      year={2023},
      booktitle={Advances in Neural Information Processing Systems (NeurIPS) 2023},
      doi={https://doi.org/10.48550/arXiv.2310.13336},
}
