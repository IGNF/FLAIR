---
title: D√©fis Flair
---
üá¨üáß <a style="font-size: 11pt" href="./index.html"><b>English version</b></a>

# Bienvenue sur la page des datasets FLAIR de l'IGN!

<p align="center"><img src="img/flair_bandeau.jpg" alt="" style="width:100%;max-width:1200px;" /></p>
<!---
[Learn more about the context of these challenges.](./why_flair.html)<br>
--->



<center>
<table style="width:100%;max-width:1400px;">
<thead>
  <tr>
    <th></th>
    <th colspan="2" style="text-align: center; padding: 2% 8% 2% 8%">L'Institut national de l'information g√©ographique et foresti√®re (IGN) pr√©sente ses d√©fis en mati√®re d'IA et ses jeux de donn√©es de r√©f√©rence FLAIR (pour French Land cover from Aerospace ImageRy). Les jeux de donn√©es FLAIR mettent √† disposition des donn√©es d'observation de la Terre provenant de diff√©rents capteurs a√©rospatiaux. Ces jeux de donn√©es couvrent de larges √©chelles et refl√®tent des cas r√©els et complexes de cartographie de l'occupation du sol.<br><br>
Nous nous engageons √† soutenir la recherche et √† favoriser l'innovation dans les domaines de l'observation de la Terre. Pour toute question concernant les donn√©es, leur acc√®s et leur exploitation, ainsi que pour toute id√©e de futurs jeux de donn√©es ou suggestion de sujets, contactez nous √† l'adresse suivante: <a href = "mailto:flair@ign.fr?subject=FLAIR @IGN">flair@ign.fr</a></th>
    <th></th>
  </tr>
</thead>
<tbody>
  <tr>
    <td></td>
    <td colspan="2" style="text-align: center; padding: 2% 4% 2% 4%"><div style="padding: 5px; border: 1px solid red; margin-bottom: 8px;">  
<code style="padding: 2px 4px; font-size: 100%; color: #c7254e;  border-radius: 0;">Explorez les jeux de donn√©es et les codes associ√©s :</code><br/>
<ul style="list-style-type:none;">
<li> <a style="font-size: 11.5pt" href="#FLAIR1">FLAIR #1 : segmentation s√©mantique d'imagerie a√©rienne</a></li>
<li> <a style="font-size: 11.5pt" href="#FLAIR2">FLAIR #2 : dataset multimodal optique et segmentation s√©mantique</a></td>
    <td></td>
  </tr>
  <tr>
    <td></td>
    <td colspan="2" style="text-align: center; padding: 2% 8% 2% 8%">Les jeux de donn√©es FLAIR sont sous la licence <a href="https://www.etalab.gouv.fr/wp-content/uploads/2018/11/open-licence.pdf" target="_blank"><b>Licence Ouverte 2.0</b></a> d'Etalab.<br>Faites mention de la paternit√© du jeu de donn√©es en citant le datapapers associ√©.<br><br>
    <img style="width:100%;max-width:200px;" class="center-block" src="img/ETALAB_LICENCE_FLAIR.png"><br></td>
    <td></td>
  </tr>
  <tr>
    <td colspan="4" style="text-align: center;"><img src="img/FLAIR_recap_3.png" alt="" style="width:100%;max-width:800px;" /></td>
  </tr>
  <tr>
    <td colspan="4" style="text-align: center;"><img src="img/flair_dataset_illustration_dark-min.png" alt="" style="width:100%;max-width:800px;"/></td>
  </tr>
</tbody>
</table>
</center>


<hr>
<br><br/><br/><br/><br/><br/><br/><br/><br/><a id="FLAIR1"></a>



## FLAIR #1 : segmentation s√©mantique et adaptation de domaine üåçüå±üè†üå≥‚û°Ô∏èüõ©Ô∏è


Challenge organis√© par l'IGN avec le soutient de la <a href="https://www.sfpt.fr/" target="_blank"><b>SFPT.</b></a>
Ce challenge s'est d√©roul√© du 21 Novembre 2022 au 21 Mars 2023. Vous pouvez consulter les r√©sultats <a style="font-size: 10pt" href="https://codalab.lisn.upsaclay.fr/competitions/8769" target="_blank"><b>ici.</b></a><br/><br/><br/>

>FLAIR #1 <b> datapaper  &#128209;</b>  : https://arxiv.org/pdf/2211.12979.pdf <br/>
>FLAIR #1 <b> d√©p√¥t github &#128193;</b> : https://github.com/IGNF/FLAIR-1-AI-Challenge

><b>Mod√®les pr√©-entra√Æn√©s</b> : https://huggingface.co/collections/IGNF/flair-models-landcover-semantic-segmentation-65bb67415a5dbabc819a95de

<br/><br/>



<details><summary><font size=3.5>‚ñ∂Ô∏è Description du dataset</font> <em>(cliquer pour agrandir)</em></summary> 

Avec des donn√©es acquises sur 50 d√©partements et plus de 20 milliards de pixels annot√©s, ce jeu de donn√©es repr√©sente la diversit√© du territoire m√©tropolitain, ses climats, ses √©cosyst√®mes et ses sols, dans le but de produire une cartographie √† grande √©chelle. Diff√©rentes bases de donn√©es IGN (BD Ortho, RGE Alti) ainsi que des annotations produites manuellement par des experts photo-interpr√®tes ont √©t√© assembl√©es pour permettre l‚Äôentra√Ænement de mod√®les IA.

Les images a√©riennes de t√©l√©d√©tection √† l'√©chelle d'un pays sont n√©cessairement acquises √† des dates et des heures diff√©rentes et dans des conditions diff√©rentes. De m√™me, √† grande √©chelle, les caract√©ristiques des classes s√©mantiques peuvent varier et devenir h√©t√©rog√®nes. Cela soul√®ve des challenges pour la g√©n√©ralisation spatiale et temporelle des mod√®les d'apprentissage profond !

Le dataset FLAIR#1 est compos√© de 77,412 patches de 512x512 (r√©solution spatiale de 0.2m) avec une s√©mantique √† 19 classes. Sp√©cifiquement pour le challenge et les baselines associ√©es et en raison d'une fr√©quence par classe d√©s√©quilibr√©e, la s√©mantique a √©t√© modifi√©e √† 13 classes (>12 -> 13). Rapportez-vous au datapaper pour plus de pr√©cisions.<br/>

<center>
<table style="width:80%;max-width:700px;">
<thead>
  <tr><th width=7%></th><th>Classe</th><th style='text-align: center' width=15%>Valeur</th><th style='text-align: center'>Freq.-entra√Ænement (%)</th><th style='text-align: center'>Freq.-test (%)</th></tr>
</thead>
<tbody>
  <tr><td bgcolor='#db0e9a'></td><td>b√¢timent</td><td style='text-align: center'>1</td><td style='text-align: center'>8.14</td><td style='text-align: center'>8.6</td></tr>
  
  <tr><td bgcolor='#938e7b'></td><td>zone perm√©able</td><td style='text-align: center'>2</td><td style='text-align: center'>8.25</td><td style='text-align: center'>7.34</td></tr>
  
  <tr><td bgcolor='#f80c00'></td><td>zone imperm√©able</td><td style='text-align: center'>3</td><td style='text-align: center'>13.72</td><td style='text-align: center'>14.98</td></tr>
  
  <tr><td bgcolor='#a97101'></td><td>sol nu</td><td style='text-align: center'>4</td><td style='text-align: center'>3.47</td><td style='text-align: center'>4.36</td></tr>
  
  <tr><td bgcolor='#1553ae'></td><td>eau</td><td style='text-align: center'>5</td><td style='text-align: center'>4.88</td><td style='text-align: center'>5.98</td></tr>
  
  <tr><td bgcolor='#194a26'></td><td>conif√®res</td><td style='text-align: center'>6</td><td style='text-align: center'>2.74</td><td style='text-align: center'>2.39</td></tr>
  
  <tr><td bgcolor='#46e483'></td><td>feuillus</td><td style='text-align: center'>7</td><td style='text-align: center'>15.38</td><td style='text-align: center'>13.91</td></tr>
  
  <tr><td bgcolor='#f3a60d'></td><td>brousaille</td><td style='text-align: center'>8</td><td style='text-align: center'>6.95</td><td style='text-align: center'>6.91</td></tr>
  
  <tr><td bgcolor='#660082'></td><td>vigne</td><td style='text-align: center'>9</td><td style='text-align: center'>3.13</td><td style='text-align: center'>3.87</td></tr>
  
  <tr><td bgcolor='#55ff00'></td><td>pelouse</td><td style='text-align: center'>10</td><td style='text-align: center'>17.84</td><td style='text-align: center'>22.17</td></tr>
  
  <tr><td bgcolor='#fff30d'></td><td>culture</td><td style='text-align: center'>11</td><td style='text-align: center'>10.98</td><td style='text-align: center'>6.95</td></tr>
  
  <tr><td bgcolor='#e4df7c'></td><td>terre labour√©e</td><td style='text-align: center'>12</td><td style='text-align: center'>3.88</td><td style='text-align: center'>2.25</td></tr>
  
  <tr><td bgcolor='#3de6eb'></td><td>piscine</td><td style='text-align: center'>13</td><td style='text-align: center'>0.03</td><td style='text-align: center'>0.04</td></tr>
  
  <tr><td bgcolor='#ffffff'></td><td>neige</td><td style='text-align: center'>14</td><td style='text-align: center'>0.15</td><td style='text-align: center'>-</td></tr>
  
  <tr><td bgcolor='#8ab3a0'></td><td>coupe</td><td style='text-align: center'>15</td><td style='text-align: center'>0.15</td><td style='text-align: center'>0.01</td></tr>
  
  <tr><td bgcolor='#6b714f'></td><td>mixte</td><td style='text-align: center'>16</td><td style='text-align: center'>0.05</td><td style='text-align: center'>-</td></tr>
  
  <tr><td bgcolor='#c5dc42'></td><td>ligneux</td><td style='text-align: center'>17</td><td style='text-align: center'>0.01</td><td style='text-align: center'>0.03</td></tr>
  
  <tr><td bgcolor='#9999ff'></td><td>serres</td><td style='text-align: center'>18</td><td style='text-align: center'>0.12</td><td style='text-align: center'>0.2</td></tr>
  
  <tr><td bgcolor='#000000'></td><td>autre</td><td style='text-align: center'>19</td><td style='text-align: center'>0.14</td><td style='text-align: center'>-</td></tr>
</tbody>
</table>
</center>


Le dataset couvre un total d'environ 812 km¬≤, avec des patches s√©lectionn√©s sur l'ensemble du territoire m√©tropolitain afin de repr√©senter sa diversit√© (domaines spatiaux). Les images a√©riennes incluent dans le dataset sont √©galement acquisent √† des mois et ann√©es diff√©rentes (domaines temporels). 


<table>
    <tr>
        <td style="text-align: center"><img src="img/flair_ortho.png" 
            alt="Image a√©rienne ORTHO HR" title="ORTHO HR" /></td>
        <td style="text-align: center"><img src="img/flair_labels.png" alt="Labels" 
            title="Annotations" /></td>
    </tr>
    <tr>
        <td style="text-align: center">Image a√©rienne ORTHO HR&#174;</td>
        <td style="text-align: center">Annotations</td>
    </tr>
</table>


Le dataset de test contient 15,700 patches de 10 domaines spatiaux suppl√©mentaires. La fr√©quence des classes et les domaines temporels sont √©galement distinct du dataset d'entra√Ænement, permettant d'analyser les capacit√©s de g√©n√©ralisation et d'adaptation de domaines des m√©thodes d√©velopp√©es.<br/><br/>
</details>
<br>


<details><summary><font size=3.5>‚ñ∂Ô∏è Mod√®le de r√©f√©rence (baseline): U-Net</font>  <em>(cliquer pour agrandir)</em></summary> 

Une architecture U-Net avec un encodeur ResNet34 pr√©-entra√Æn√© de la librairie segmentation-models-pytorch a √©t√© utilis√©e pour les baselines. L'architecture utilis√©e permet l'int√©gration d'informations de m√©tadonn√©es √† l'√©chelle du patch et utilise des techniques d'augmentation des donn√©es d'image couramment utilis√©es. Les codes sont disponibles dans le d√©p√¥t FLAIR #1.<br/><br/>

<p align="center"><img src="img/flair_predict.gif" alt="" style="width:100%;max-width:850px;" /></p>
<br/><br/>
</details>
<br>





<details><summary><font size=3.5>‚ñ∂Ô∏è T√©l√©chargement du dataset</font> <em>(cliquer pour agrandir)</em></summary> 


<center>
<table style="width:80%;max-width:700px;">
  <tr>
    <th>Donn√©es</th>
    <th>Volume</th>
    <th>Type</th>
    <th>Lien</th>
  </tr>
  <tr><td colspan="4" height = 10px></td>
  <tr>
    <td>Images a√©riennes - entra√Ænement</td>
    <td>50.7 Go</td>
    <td>.zip</td>
    <td><a style="font-size: 10pt" href="https://storage.gra.cloud.ovh.net/v1/AUTH_366279ce616242ebb14161b7991a8461/defi-ia/flair_data_1/flair_aerial_train.zip"><b>t√©l√©chargement</b></a>
  </tr>
  <tr>
    <td>Images a√©riennes - test</td>
    <td>13.4 Go</td>
    <td>.zip</td>
    <td><a style="font-size: 10pt" href="https://storage.gra.cloud.ovh.net/v1/AUTH_366279ce616242ebb14161b7991a8461/defi-ia/flair_data_1/flair_1_aerial_test.zip"><b>t√©l√©chargement</b></a>
  </tr>
  <tr>
    <td>Annotations - entra√Ænement</td>
    <td>485 Mo</td>
    <td>.zip</td>
    <td><a style="font-size: 10pt" href="https://storage.gra.cloud.ovh.net/v1/AUTH_366279ce616242ebb14161b7991a8461/defi-ia/flair_data_1/flair_labels_train.zip"><b>t√©l√©chargement</b></a>
  </tr>
  <tr>
    <td>Annotations - test</td>
    <td>124 Mo</td>
    <td>.zip</td>
    <td><a style="font-size: 10pt" href="https://storage.gra.cloud.ovh.net/v1/AUTH_366279ce616242ebb14161b7991a8461/defi-ia/flair_data_1/flair_1_labels_test.zip"><b>t√©l√©chargement</b></a>
  </tr>
  <tr><td colspan="4" height = 10px></td>
  <tr>
    <td>M√©tadonn√©es a√©riennes</td>
    <td>16.1 Mo</td>
    <td>.json</td>
    <td><a style="font-size: 10pt" href="https://storage.gra.cloud.ovh.net/v1/AUTH_366279ce616242ebb14161b7991a8461/defi-ia/flair_data_1/flair-1_metadata_aerial.zip"><b>t√©l√©chargement</b></a>
  </tr>
  <tr>
    <td>Shapefile zones</td>
    <td>392 Ko</td>
    <td>.gpkg</td>
    <td><a style="font-size: 10pt" href="https://storage.gra.cloud.ovh.net/v1/AUTH_366279ce616242ebb14161b7991a8461/defi-ia/flair_data_1/flair_1_shapes.gpkg"><b>t√©l√©chargement</b></a>
  </tr>
  <tr><td colspan="4" height = 10px></td>
  <tr>
    <td>Jeu de donn√©es exemple (entra√Ænement et test r√©duits)</td>
    <td>215 Mo</td>
    <td>.zip</td>
    <td><a style="font-size: 10pt" href="https://storage.gra.cloud.ovh.net/v1/AUTH_366279ce616242ebb14161b7991a8461/defi-ia/flair_data_1/flair_1_toy_dataset.zip"><b>t√©l√©chargement</b></a>
  </tr>
</table>
</center>
<br/><br/>
</details>

<br><br>


### Citation

Si vous utilisez des donn√©es de FLAIR #1, merci d'inclure la citation suivante:

Texte brut:
```
Anatol Garioud, St√©phane Peillet, Eva Bookjans, S√©bastien Giordano, and Boris Wattrelos. 2022. 
FLAIR #1: semantic segmentation and domain adaptation dataset. (2022). 
DOI:https://doi.org/10.13140/RG.2.2.30183.73128/1
```

BibTex:
```
@article{ign2022flair1,
  doi = {10.13140/RG.2.2.30183.73128/1},
  url = {https://arxiv.org/pdf/2211.12979.pdf},
  author = {Garioud, Anatol and Peillet, St√©phane and Bookjans, Eva and Giordano, S√©bastien and Wattrelos, Boris},
  title = {FLAIR #1: semantic segmentation and domain adaptation dataset},
  publisher = {arXiv},
  year = {2022}
}
```













<br><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><a id="FLAIR2"></a>


## FLAIR #2 : Information texturale et temporelle √† partir d'imagerie optique multimodal pour la segmentation s√©mantique üåçüå±üè†üå≥‚û°Ô∏èüõ©Ô∏èüõ∞Ô∏è
 
Challenge organis√© par l'IGN avec le soutient du <a href="https://cnes.fr/en" target="_blank"><b>CNES</b></a> et de <a href="https://www.connectbycnes.fr/" target="_blank"><b>Connect by CNES</b></a> dans le cadre d'un projet Copernicus / FPCUP.<br>
<img style="width:100%;max-width:800px;" src="img/flair-2_logos-hd.png" alt="Drawing"/>
<br/><br/><br/>

>FLAIR #2 <b>datapaper  &#128209;</b>  : https://arxiv.org/pdf/2305.14467.pdf <br/>
>FLAIR #2 <b>NeurIPS datapaper  &#128209;</b>  : https://proceedings.neurips.cc/paper_files/paper/2023/file/353ca88f722cdd0c481b999428ae113a-Paper-Datasets_and_Benchmarks.pdf <br/>
>FLAIR #2 <b>NeurIPS poster  &#128209;</b>  : https://neurips.cc/media/PosterPDFs/NeurIPS%202023/73621.png?t=1699528363.252194 <br/>
>FLAIR #2 <b>d√©p√¥t GitHub &#128193;</b> : https://github.com/IGNF/FLAIR-2-AI-Challenge <br/>
>FLAIR #2 <b>page du d√©fi &#128187;</b> : https://codalab.lisn.upsaclay.fr/competitions/13447 [termin√©]

><b>Mod√®les pr√©-entra√Æn√©s</b> : pour l'instant sur demande ! 

<br/><br/>


<details><summary><font size=3.5>‚ñ∂Ô∏è Contexte du challenge</font><em> (cliquer pour agrandir)</em></summary> 

Avec ce nouveau d√©fi, les participants auront pour t√¢che de d√©velopper des solutions innovantes qui peuvent exploiter efficacement les informations texturales des images a√©riennes prises √† une seule date, ainsi que les informations temporelles/spectrales provenant des s√©ries temporelles des satellites Sentinel-2, afin d'am√©liorer la segmentation s√©mantique, l'adaptation de domaine et l'apprentissage par transfert. Vos solutions devraient relever les d√©fis li√©s √† la conciliation des diff√©rentes p√©riodes d'acquisition, des r√©solutions spatiales diff√©rentes, de l'adaptation aux conditions d'acquisitions variables et de la gestion de l'h√©t√©rog√©n√©it√© des classes s√©mantiques.


<br/><br/>
<p align="center"><img src="img/flair_2_frise.png" alt="" style="width:100%;max-width:1000px;"/></p><br>

</details>
<br>

<details><summary><font size=3.5>‚ñ∂Ô∏è Description du dataset</font> <em> (cliquer pour agrandir)</em></summary> 

Le jeu de donn√©es FLAIR #2 comprend 20,384,841,728 pixels annot√©s avec une r√©solution spatiale de 0,20 m √† partir d'images a√©riennes, r√©partis en 77,762 patchs de taille 512x512. Le jeu de donn√©es FLAIR #2 inclut √©galement une vaste collection de donn√©es satellites, avec un total de 51,244 acquisitions d'images des satellites Copernicus Sentinel-2. Pour chaque zone, les acquisitions sur un an ont √©t√© retenues, offrant des informations pr√©cieuses sur la dynamique spatio-temporelle et les caract√©ristiques spectrales de la couverture terrestre. En raison de la diff√©rence significative de r√©solution spatiale entre les images a√©riennes et les donn√©es satellites, les zones initialement d√©finies manquent de contexte suffisant car elles ne sont compos√©es que de quelques pixels Sentinel-2. Pour rem√©dier √† cela, une marge a √©t√© appliqu√©e pour cr√©er des zones plus grandes appel√©es super-zones. Cela garantit que chaque patch du jeu de donn√©es est associ√© √† une super-zone de donn√©es Sentinel-2 de taille suffisante, offrant un niveau minimum de contexte provenant du satellite.<br><br/>

<p align="center"><img src="img/flair_2_spatial.png" alt="" style="width:100%;max-width:400px;"/></p><br>

Le jeu de donn√©es couvre 50 domaines spatial, comprenant 916 zones r√©parties sur 817 km¬≤. Avec 13 classes s√©mantiques (plus 6 non utilis√©es dans ce d√©fi), ce jeu de donn√©es constitue une base solide pour faire progresser les techniques de cartographie de la couverture du sol.<br><br>

<center>
<table style="width:80%;max-width:700px;">
<thead>
  <tr><th width=7%></th><th>Class</th><th style='text-align: center' width=15%>Value</th><th style='text-align: center'>Freq.-train (%)</th><th style='text-align: center'>Freq.-test (%)</th></tr>
</thead>
<tbody>
  <tr><td bgcolor='#db0e9a'></td><td>building</td><td style='text-align: center'>1</td><td style='text-align: center'>8.14</td><td style='text-align: center'>3.26</td></tr>
  
  <tr><td bgcolor='#938e7b'></td><td>pervious surface</td><td style='text-align: center'>2</td><td style='text-align: center'>8.25</td><td style='text-align: center'>3.82</td></tr>
  
  <tr><td bgcolor='#f80c00'></td><td>impervious surface</td><td style='text-align: center'>3</td><td style='text-align: center'>13.72</td><td style='text-align: center'>5.87</td></tr>
  
  <tr><td bgcolor='#a97101'></td><td>bare soil</td><td style='text-align: center'>4</td><td style='text-align: center'>3.47</td><td style='text-align: center'>1.6</td></tr>
  
  <tr><td bgcolor='#1553ae'></td><td>water</td><td style='text-align: center'>5</td><td style='text-align: center'>4.88</td><td style='text-align: center'>3.17</td></tr>
  
  <tr><td bgcolor='#194a26'></td><td>coniferous</td><td style='text-align: center'>6</td><td style='text-align: center'>2.74</td><td style='text-align: center'>10.24</td></tr>
  
  <tr><td bgcolor='#46e483'></td><td>deciduous</td><td style='text-align: center'>7</td><td style='text-align: center'>15.38</td><td style='text-align: center'>24.79</td></tr>
  
  <tr><td bgcolor='#f3a60d'></td><td>brushwood</td><td style='text-align: center'>8</td><td style='text-align: center'>6.95</td><td style='text-align: center'>3.81</td></tr>
  
  <tr><td bgcolor='#660082'></td><td>vineyard</td><td style='text-align: center'>9</td><td style='text-align: center'>3.13</td><td style='text-align: center'>2.55</td></tr>
  
  <tr><td bgcolor='#55ff00'></td><td>herbaceous vegetation</td><td style='text-align: center'>10</td><td style='text-align: center'>17.84</td><td style='text-align: center'>19.76</td></tr>
  
  <tr><td bgcolor='#fff30d'></td><td>agricultural land</td><td style='text-align: center'>11</td><td style='text-align: center'>10.98</td><td style='text-align: center'>18.19</td></tr>
  
  <tr><td bgcolor='#e4df7c'></td><td>plowed land</td><td style='text-align: center'>12</td><td style='text-align: center'>3.88</td><td style='text-align: center'>1.81</td></tr>
  
  <tr><td bgcolor='#3de6eb'></td><td>swimming pool</td><td style='text-align: center'>13</td><td style='text-align: center'>0.01</td><td style='text-align: center'>0.02</td></tr>
  
  <tr><td bgcolor='#ffffff'></td><td>snow</td><td style='text-align: center'>14</td><td style='text-align: center'>0.15</td><td style='text-align: center'>-</td></tr>
  
  <tr><td bgcolor='#8ab3a0'></td><td>clear cut</td><td style='text-align: center'>15</td><td style='text-align: center'>0.15</td><td style='text-align: center'>0.82</td></tr>
  
  <tr><td bgcolor='#6b714f'></td><td>mixed</td><td style='text-align: center'>16</td><td style='text-align: center'>0.05</td><td style='text-align: center'>0.12</td></tr>
  
  <tr><td bgcolor='#c5dc42'></td><td>ligneous</td><td style='text-align: center'>17</td><td style='text-align: center'>0.01</td><td style='text-align: center'>-</td></tr>
  
  <tr><td bgcolor='#9999ff'></td><td>greenhouse</td><td style='text-align: center'>18</td><td style='text-align: center'>0.12</td><td style='text-align: center'>0.15</td></tr>
  
  <tr><td bgcolor='#000000'></td><td>other</td><td style='text-align: center'>19</td><td style='text-align: center'>0.14</td><td style='text-align: center'>0.04</td></tr>
</tbody>
</table>
</center>
<br><br>
</details>
<br>

<details><summary><font size=3.5>‚ñ∂Ô∏è Mod√®le de r√©f√©rence (baseline): U-T&T</font> <em> (cliquer pour agrandir)</em></summary> 

Nous proposons le mod√®le U-T&T, une architecture √† deux branches qui combine les informations spatiales et temporelles √† partir d'images a√©riennes tr√®s haute r√©solution et d'images satellites haute r√©solution en une seule sortie. L'architecture U-Net est utilis√©e pour la branche spatiale/texture, en utilisant un mod√®le avec un encodeur ResNet34 pr√©-entra√Æn√© sur ImageNet. Pour la branche spatio-temporelle, l'architecture U-TAE int√®gre un Encodeur √† Attention Temporelle (TAE) pour explorer les caract√©ristiques spatiales et temporelles des s√©ries temporelles de Sentinel-2, en appliquant des masques d'attention √† diff√©rentes r√©solutions lors du d√©codage. Ce mod√®le permet la fusion des informations apprises √† partir des deux sources.
<br><br>
</details>
<br>

<details><summary><font size=3.5>‚ñ∂Ô∏è T√©l√©chargement du dataset</font> <em>(cliquer pour agrandir)</em></summary> 

Pour l'instant sur inscription au d√©fi ! <br>

<center>
<table style="width:80%;max-width:700px;">
  <tr>
    <th>Donn√©es</th><th>Volume</th><th>Type</th><th>Lien</th>
 </tr>
 <tr><td colspan="4" height = 10px></td>
 </tr>
  <tr>
    <td>Images a√©riennes - entra√Ænement</td><td>50.7 Go</td><td>.zip</td>
    <td>-</td>
  </tr>
   <tr>
    <td>Images a√©riennes - test</td><td>13.4 Go</td><td>.zip</td>
    <td>-</td>
  </tr>
 </tr>
  <tr>
    <td>Images Sentinel-2 - entra√Ænement</td><td>22.8 Go</td><td>.zip</td>
    <td>-</td>
  </tr>
   <tr>
    <td>Images Sentinel-2 - test</td><td>6 Go</td><td>.zip</td>
    <td>-</td>
  </tr>
  <tr>
    <td>Annotations - entra√Ænement</td><td>485 Mo</td><td>.zip</td>
    <td>-</td>
  </tr>
  <tr style="cellspacing: 10px">
    <td>Annotations - test</td><td>108 Mo</td><td>.zip</td>
    <td>-</td>
  </tr>
  <tr><td colspan="4" height=10px style="outline: "></td>
  <tr>
    <td>M√©tadonn√©es a√©riennes</td><td>16.1 Mo</td><td>.json</td>
    <td>-</td>
  </tr>
  <tr>
    <td>Correspondance images a√©riennes <-> Sentinel-2</td><td>16.1 Mo</td><td>.json</td>
    <td>-</td>
  </tr>
  <tr>
    <td>Shapefile zones</td><td>392 Ko</td><td>.gpkg</td>
    <td>-</td>
  </tr>
  <tr><td colspan="4" height=10px style="outline: "></td>
  <tr>
    <td>Jeu de donn√©es exemple (entra√Ænement et test r√©duits)</td><td>1.6 Go/td><td>.zip</td>
    <td><a style="font-size: 10pt" href="https://storage.gra.cloud.ovh.net/v1/AUTH_366279ce616242ebb14161b7991a8461/defi-ia/flair_data_2/flair_2_toy_dataset.zip"><b>download</b></a></td>
  </tr>
</table>
</center>
<br/><br/>

</details>
<br><br>

### Citation

Si vous utilisez des donn√©es de FLAIR #2, merci d'inclure la citation suivante:

Texte brut:
```
Anatol Garioud, Nicolas Gonthier, Loic Landrieu, Apolline De Wit, Marion Valette, Marc Poup√©e, S√©bastien Giordano and Boris Wattrelos. 2023. 
FLAIR: a Country-Scale Land Cover Semantic Segmentation Dataset From Multi-Source Optical Imagery. (2023). 
DOI: https://doi.org/10.48550/arXiv.2310.13336
```

BibTex:
```
@inproceedings{ign2023flair2,
      title={FLAIR: a Country-Scale Land Cover Semantic Segmentation Dataset From Multi-Source Optical Imagery}, 
      author={Anatol Garioud and Nicolas Gonthier and Loic Landrieu and Apolline De Wit and Marion Valette and Marc Poup√©e and S√©bastien Giordano and Boris Wattrelos},
      year={2023},
      booktitle={Advances in Neural Information Processing Systems (NeurIPS) 2023},
      doi={https://doi.org/10.48550/arXiv.2310.13336},
}
